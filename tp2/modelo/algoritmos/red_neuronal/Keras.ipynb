{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import  MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "\n",
    "from keras import backend\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_installs = pd.read_csv(\"../../features/entrenar_installs_final.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_installs.fillna(0,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train_installs.drop(\"target\",axis = 1), train_installs[\"target\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler(copy=True, feature_range=(0, 1))\n",
      "MinMaxScaler(copy=True, feature_range=(0, 1))\n"
     ]
    }
   ],
   "source": [
    "y= y.values.reshape(-1,1)\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "print(scaler_x.fit(X))\n",
    "xscale=scaler_x.transform(X)\n",
    "print(scaler_y.fit(y))\n",
    "yscale=scaler_y.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(xscale, yscale, test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def build_regressor():\n",
    "    regressor = Sequential()\n",
    "    regressor.add(Dense(units=13, input_dim=104))\n",
    "    regressor.add(Dense(units=1))\n",
    "    regressor.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 12)                1524      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 52        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 1,581\n",
      "Trainable params: 1,581\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=126, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='adam', metrics=['mse',rmse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 457968 samples, validate on 225567 samples\n",
      "Epoch 1/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0856 - mean_squared_error: 0.0856 - rmse: 0.2535 - val_loss: 0.0849 - val_mean_squared_error: 0.0849 - val_rmse: 0.2541\n",
      "Epoch 2/100\n",
      "457968/457968 [==============================] - 4s 10us/step - loss: 0.0849 - mean_squared_error: 0.0849 - rmse: 0.2530 - val_loss: 0.0848 - val_mean_squared_error: 0.0848 - val_rmse: 0.2515\n",
      "Epoch 3/100\n",
      "457968/457968 [==============================] - 5s 10us/step - loss: 0.0848 - mean_squared_error: 0.0848 - rmse: 0.2529 - val_loss: 0.0846 - val_mean_squared_error: 0.0846 - val_rmse: 0.2533\n",
      "Epoch 4/100\n",
      "457968/457968 [==============================] - 5s 10us/step - loss: 0.0848 - mean_squared_error: 0.0848 - rmse: 0.2529 - val_loss: 0.0846 - val_mean_squared_error: 0.0846 - val_rmse: 0.2520\n",
      "Epoch 5/100\n",
      "457968/457968 [==============================] - 4s 9us/step - loss: 0.0847 - mean_squared_error: 0.0847 - rmse: 0.2530 - val_loss: 0.0846 - val_mean_squared_error: 0.0846 - val_rmse: 0.2534\n",
      "Epoch 6/100\n",
      "457968/457968 [==============================] - 4s 9us/step - loss: 0.0847 - mean_squared_error: 0.0847 - rmse: 0.2529 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2526\n",
      "Epoch 7/100\n",
      "457968/457968 [==============================] - 4s 9us/step - loss: 0.0847 - mean_squared_error: 0.0847 - rmse: 0.2529 - val_loss: 0.0848 - val_mean_squared_error: 0.0848 - val_rmse: 0.2541\n",
      "Epoch 8/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0847 - mean_squared_error: 0.0847 - rmse: 0.2529 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2527\n",
      "Epoch 9/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0847 - mean_squared_error: 0.0847 - rmse: 0.2529 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2524\n",
      "Epoch 10/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0847 - mean_squared_error: 0.0847 - rmse: 0.2529 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2525\n",
      "Epoch 11/100\n",
      "457968/457968 [==============================] - 4s 9us/step - loss: 0.0847 - mean_squared_error: 0.0847 - rmse: 0.2529 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2526\n",
      "Epoch 12/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0847 - mean_squared_error: 0.0847 - rmse: 0.2529 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2530\n",
      "Epoch 13/100\n",
      "457968/457968 [==============================] - 5s 12us/step - loss: 0.0847 - mean_squared_error: 0.0847 - rmse: 0.2529 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2522\n",
      "Epoch 14/100\n",
      "457968/457968 [==============================] - 5s 10us/step - loss: 0.0847 - mean_squared_error: 0.0847 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2531\n",
      "Epoch 15/100\n",
      "457968/457968 [==============================] - 5s 11us/step - loss: 0.0847 - mean_squared_error: 0.0847 - rmse: 0.2528 - val_loss: 0.0850 - val_mean_squared_error: 0.0850 - val_rmse: 0.2542\n",
      "Epoch 16/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0847 - mean_squared_error: 0.0847 - rmse: 0.2529 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2524\n",
      "Epoch 17/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0847 - mean_squared_error: 0.0847 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2525\n",
      "Epoch 18/100\n",
      "457968/457968 [==============================] - 4s 9us/step - loss: 0.0847 - mean_squared_error: 0.0847 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2529\n",
      "Epoch 19/100\n",
      "457968/457968 [==============================] - 4s 9us/step - loss: 0.0847 - mean_squared_error: 0.0847 - rmse: 0.2528 - val_loss: 0.0846 - val_mean_squared_error: 0.0846 - val_rmse: 0.2532\n",
      "Epoch 20/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0847 - mean_squared_error: 0.0847 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2528\n",
      "Epoch 21/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0847 - mean_squared_error: 0.0847 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2529\n",
      "Epoch 22/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0847 - mean_squared_error: 0.0847 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2524\n",
      "Epoch 23/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0847 - mean_squared_error: 0.0847 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2527\n",
      "Epoch 24/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0847 - mean_squared_error: 0.0847 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2525\n",
      "Epoch 25/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0847 - mean_squared_error: 0.0847 - rmse: 0.2528 - val_loss: 0.0846 - val_mean_squared_error: 0.0846 - val_rmse: 0.2535\n",
      "Epoch 26/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0847 - mean_squared_error: 0.0847 - rmse: 0.2528 - val_loss: 0.0846 - val_mean_squared_error: 0.0846 - val_rmse: 0.2518\n",
      "Epoch 27/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0847 - mean_squared_error: 0.0847 - rmse: 0.2528 - val_loss: 0.0846 - val_mean_squared_error: 0.0846 - val_rmse: 0.2517\n",
      "Epoch 28/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0847 - mean_squared_error: 0.0847 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2526\n",
      "Epoch 29/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0847 - mean_squared_error: 0.0847 - rmse: 0.2528 - val_loss: 0.0846 - val_mean_squared_error: 0.0846 - val_rmse: 0.2517\n",
      "Epoch 30/100\n",
      "457968/457968 [==============================] - 4s 9us/step - loss: 0.0847 - mean_squared_error: 0.0847 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2525\n",
      "Epoch 31/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2528 - val_loss: 0.0847 - val_mean_squared_error: 0.0847 - val_rmse: 0.2537\n",
      "Epoch 32/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2529\n",
      "Epoch 33/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2527\n",
      "Epoch 34/100\n",
      "457968/457968 [==============================] - 4s 9us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2525\n",
      "Epoch 35/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2530\n",
      "Epoch 36/100\n",
      "457968/457968 [==============================] - 5s 11us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2522\n",
      "Epoch 37/100\n",
      "457968/457968 [==============================] - 4s 9us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2523\n",
      "Epoch 38/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2528 - val_loss: 0.0846 - val_mean_squared_error: 0.0846 - val_rmse: 0.2519\n",
      "Epoch 39/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2522\n",
      "Epoch 40/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2524\n",
      "Epoch 41/100\n",
      "457968/457968 [==============================] - 4s 9us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2524\n",
      "Epoch 42/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2527\n",
      "Epoch 43/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2525\n",
      "Epoch 44/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2528\n",
      "Epoch 45/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2525\n",
      "Epoch 46/100\n",
      "457968/457968 [==============================] - 4s 9us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2532\n",
      "Epoch 47/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2530\n",
      "Epoch 48/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2527 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2527\n",
      "Epoch 49/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2527\n",
      "Epoch 50/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2522\n",
      "Epoch 51/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2522\n",
      "Epoch 52/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2527 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2526\n",
      "Epoch 53/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2528 - val_loss: 0.0846 - val_mean_squared_error: 0.0846 - val_rmse: 0.2518\n",
      "Epoch 54/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2521\n",
      "Epoch 55/100\n",
      "457968/457968 [==============================] - 4s 9us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2525\n",
      "Epoch 56/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2526\n",
      "Epoch 57/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2528 - val_loss: 0.0846 - val_mean_squared_error: 0.0846 - val_rmse: 0.2518\n",
      "Epoch 58/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2531\n",
      "Epoch 59/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2525\n",
      "Epoch 60/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2527\n",
      "Epoch 61/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2527 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2530\n",
      "Epoch 62/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2527 - val_loss: 0.0846 - val_mean_squared_error: 0.0846 - val_rmse: 0.2534\n",
      "Epoch 63/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2527 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2521\n",
      "Epoch 64/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2522\n",
      "Epoch 65/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2527 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2522\n",
      "Epoch 66/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2527 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2530\n",
      "Epoch 67/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2527 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2521\n",
      "Epoch 68/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2527\n",
      "Epoch 69/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2527 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2522\n",
      "Epoch 70/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2528 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2526\n",
      "Epoch 71/100\n",
      "457968/457968 [==============================] - 4s 9us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2527 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2525\n",
      "Epoch 72/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2527 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2526\n",
      "Epoch 73/100\n",
      "457968/457968 [==============================] - 4s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2527 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2526\n",
      "Epoch 74/100\n",
      "457968/457968 [==============================] - 4s 9us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2527 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2524\n",
      "Epoch 75/100\n",
      "457968/457968 [==============================] - 3s 8us/step - loss: 0.0846 - mean_squared_error: 0.0846 - rmse: 0.2527 - val_loss: 0.0845 - val_mean_squared_error: 0.0845 - val_rmse: 0.2526\n",
      "Epoch 76/100\n",
      "246500/457968 [===============>..............] - ETA: 1s - loss: 0.0847 - mean_squared_error: 0.0847 - rmse: 0.2528"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-adde6c05791e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/opt/anaconda/3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/3/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         if (self._delta_t_batch > 0. and\n\u001b[1;32m    119\u001b[0m            (delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1)):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100, batch_size=100,  verbose=1, validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_mean_squared_error', 'val_rmse', 'loss', 'mean_squared_error', 'rmse'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred= model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = scaler_y.inverse_transform(y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = scaler_y.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE value is: 66728.87139367279\n"
     ]
    }
   ],
   "source": [
    "error = sqrt(mean_squared_error(pred,real))\n",
    "\n",
    "print('RMSE value is:', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "real2 = []\n",
    "for valor in real:\n",
    "    real2.append(valor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = []\n",
    "for valor in pred:\n",
    "    pred2.append(valor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE value is: 66624.13320062519\n"
     ]
    }
   ],
   "source": [
    "error = sqrt(mean_squared_error(pred2,real2))\n",
    "\n",
    "print('RMSE value is:', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>119382.898438</td>\n",
       "      <td>53933.877550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>119382.898438</td>\n",
       "      <td>156688.812692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104541.953125</td>\n",
       "      <td>50097.187391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59367.253906</td>\n",
       "      <td>2572.345044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61204.203125</td>\n",
       "      <td>80772.848565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>102482.593750</td>\n",
       "      <td>93370.124332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>119382.898438</td>\n",
       "      <td>149232.166599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>119382.898438</td>\n",
       "      <td>244090.349802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>91305.718750</td>\n",
       "      <td>15812.252519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>119382.898438</td>\n",
       "      <td>99356.838754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>31441.652344</td>\n",
       "      <td>5353.713809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>54415.195312</td>\n",
       "      <td>3445.651269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-173.809509</td>\n",
       "      <td>2325.194820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7133.557129</td>\n",
       "      <td>531.741051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>88272.875000</td>\n",
       "      <td>12734.110221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1516.788940</td>\n",
       "      <td>131.947567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>119382.898438</td>\n",
       "      <td>182963.942958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>75833.773438</td>\n",
       "      <td>7261.235809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>91089.382812</td>\n",
       "      <td>66991.784980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>119382.898438</td>\n",
       "      <td>82475.728245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>119382.898438</td>\n",
       "      <td>13431.362088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>62764.175781</td>\n",
       "      <td>3950.591228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>25608.005859</td>\n",
       "      <td>244927.833927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>84814.828125</td>\n",
       "      <td>254865.783570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>61166.324219</td>\n",
       "      <td>10025.809610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>91719.867188</td>\n",
       "      <td>140996.323749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>39104.164062</td>\n",
       "      <td>11304.595296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>52274.511719</td>\n",
       "      <td>17349.214088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>119382.898438</td>\n",
       "      <td>232291.215384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>119382.898438</td>\n",
       "      <td>193609.668773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430435</th>\n",
       "      <td>119382.898438</td>\n",
       "      <td>24924.356025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430436</th>\n",
       "      <td>119382.898438</td>\n",
       "      <td>164205.120201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430437</th>\n",
       "      <td>119382.898438</td>\n",
       "      <td>11028.817206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430438</th>\n",
       "      <td>90496.796875</td>\n",
       "      <td>2335.296030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430439</th>\n",
       "      <td>56094.484375</td>\n",
       "      <td>230441.025318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430440</th>\n",
       "      <td>4955.689453</td>\n",
       "      <td>9821.144411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430441</th>\n",
       "      <td>79603.960938</td>\n",
       "      <td>216247.598492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430442</th>\n",
       "      <td>76589.140625</td>\n",
       "      <td>80646.159388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430443</th>\n",
       "      <td>96792.000000</td>\n",
       "      <td>56127.668504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430444</th>\n",
       "      <td>89131.421875</td>\n",
       "      <td>9899.661003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430445</th>\n",
       "      <td>70138.890625</td>\n",
       "      <td>2096.023378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430446</th>\n",
       "      <td>97446.250000</td>\n",
       "      <td>16058.010752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430447</th>\n",
       "      <td>56564.589844</td>\n",
       "      <td>24286.747478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430448</th>\n",
       "      <td>52134.601562</td>\n",
       "      <td>11539.506037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430449</th>\n",
       "      <td>30600.412109</td>\n",
       "      <td>51441.139376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430450</th>\n",
       "      <td>57255.226562</td>\n",
       "      <td>141790.447946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430451</th>\n",
       "      <td>91607.132812</td>\n",
       "      <td>77524.608704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430452</th>\n",
       "      <td>94629.531250</td>\n",
       "      <td>189846.490679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430453</th>\n",
       "      <td>90491.031250</td>\n",
       "      <td>251647.310534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430454</th>\n",
       "      <td>91241.835938</td>\n",
       "      <td>6865.639490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430455</th>\n",
       "      <td>92999.585938</td>\n",
       "      <td>47865.707975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430456</th>\n",
       "      <td>95247.382812</td>\n",
       "      <td>47288.370530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430457</th>\n",
       "      <td>100858.976562</td>\n",
       "      <td>70838.903799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430458</th>\n",
       "      <td>119382.898438</td>\n",
       "      <td>256598.554022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430459</th>\n",
       "      <td>97837.125000</td>\n",
       "      <td>165449.221638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430460</th>\n",
       "      <td>86874.976562</td>\n",
       "      <td>91078.110667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430461</th>\n",
       "      <td>55041.992188</td>\n",
       "      <td>63885.350304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430462</th>\n",
       "      <td>76595.429688</td>\n",
       "      <td>19198.038416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430463</th>\n",
       "      <td>60643.660156</td>\n",
       "      <td>240168.299976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430464</th>\n",
       "      <td>65482.390625</td>\n",
       "      <td>92993.177971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>430465 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 pred           real\n",
       "0       119382.898438   53933.877550\n",
       "1       119382.898438  156688.812692\n",
       "2       104541.953125   50097.187391\n",
       "3        59367.253906    2572.345044\n",
       "4        61204.203125   80772.848565\n",
       "5       102482.593750   93370.124332\n",
       "6       119382.898438  149232.166599\n",
       "7       119382.898438  244090.349802\n",
       "8        91305.718750   15812.252519\n",
       "9       119382.898438   99356.838754\n",
       "10       31441.652344    5353.713809\n",
       "11       54415.195312    3445.651269\n",
       "12        -173.809509    2325.194820\n",
       "13        7133.557129     531.741051\n",
       "14       88272.875000   12734.110221\n",
       "15        1516.788940     131.947567\n",
       "16      119382.898438  182963.942958\n",
       "17       75833.773438    7261.235809\n",
       "18       91089.382812   66991.784980\n",
       "19      119382.898438   82475.728245\n",
       "20      119382.898438   13431.362088\n",
       "21       62764.175781    3950.591228\n",
       "22       25608.005859  244927.833927\n",
       "23       84814.828125  254865.783570\n",
       "24       61166.324219   10025.809610\n",
       "25       91719.867188  140996.323749\n",
       "26       39104.164062   11304.595296\n",
       "27       52274.511719   17349.214088\n",
       "28      119382.898438  232291.215384\n",
       "29      119382.898438  193609.668773\n",
       "...               ...            ...\n",
       "430435  119382.898438   24924.356025\n",
       "430436  119382.898438  164205.120201\n",
       "430437  119382.898438   11028.817206\n",
       "430438   90496.796875    2335.296030\n",
       "430439   56094.484375  230441.025318\n",
       "430440    4955.689453    9821.144411\n",
       "430441   79603.960938  216247.598492\n",
       "430442   76589.140625   80646.159388\n",
       "430443   96792.000000   56127.668504\n",
       "430444   89131.421875    9899.661003\n",
       "430445   70138.890625    2096.023378\n",
       "430446   97446.250000   16058.010752\n",
       "430447   56564.589844   24286.747478\n",
       "430448   52134.601562   11539.506037\n",
       "430449   30600.412109   51441.139376\n",
       "430450   57255.226562  141790.447946\n",
       "430451   91607.132812   77524.608704\n",
       "430452   94629.531250  189846.490679\n",
       "430453   90491.031250  251647.310534\n",
       "430454   91241.835938    6865.639490\n",
       "430455   92999.585938   47865.707975\n",
       "430456   95247.382812   47288.370530\n",
       "430457  100858.976562   70838.903799\n",
       "430458  119382.898438  256598.554022\n",
       "430459   97837.125000  165449.221638\n",
       "430460   86874.976562   91078.110667\n",
       "430461   55041.992188   63885.350304\n",
       "430462   76595.429688   19198.038416\n",
       "430463   60643.660156  240168.299976\n",
       "430464   65482.390625   92993.177971\n",
       "\n",
       "[430465 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"pred\":pred2,\"real\":real2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle(\"../../../modelo/armado_datos/train_completo_auctions.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20154878421450015"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train[\"target\"].max().sum())/ len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1286041"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.fillna(0,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximo = train[\"target\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximos = train[train[\"target\"] == maximo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train[\"target\"] != maximo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.append(maximos.sample(150000,random_state = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del maximos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train.drop(\"target\",axis = 1), train[\"target\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1073544, 103)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= y.values.reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_x = MinMaxScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler(copy=True, feature_range=(0, 1))\n"
     ]
    }
   ],
   "source": [
    "scaler_y = MinMaxScaler()\n",
    "print(scaler_x.fit(X))\n",
    "xscale=scaler_x.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler(copy=True, feature_range=(0, 1))\n"
     ]
    }
   ],
   "source": [
    "print(scaler_y.fit(y))\n",
    "yscale=scaler_y.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(xscale, yscale, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def build_regressor():\n",
    "    regressor = Sequential()\n",
    "    regressor.add(Dense(units=13, input_dim=104))\n",
    "    regressor.add(Dense(units=1))\n",
    "    regressor.compile(loss='mse', optimizer='adam', metrics=[rmse])\n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0624 21:29:41.713503 140308775319360 deprecation_wrapper.py:119] From /home/pelozo/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0624 21:29:42.096714 140308775319360 deprecation_wrapper.py:119] From /home/pelozo/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0624 21:29:42.183354 140308775319360 deprecation_wrapper.py:119] From /home/pelozo/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "W0624 21:29:42.216489 140308775319360 deprecation_wrapper.py:119] From /home/pelozo/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                5200      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 408       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 5,617\n",
      "Trainable params: 5,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=103, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 21:29:42.256882 140308775319360 deprecation_wrapper.py:119] From /home/pelozo/anaconda3/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse', optimizer='adam', metrics=['mse',rmse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 21:29:42.909635 140308775319360 deprecation_wrapper.py:119] From /home/pelozo/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 858835 samples, validate on 214709 samples\n",
      "Epoch 1/100\n",
      "858835/858835 [==============================] - 20s 23us/step - loss: 0.1005 - mean_squared_error: 0.1005 - rmse: 0.2580 - val_loss: 0.0994 - val_mean_squared_error: 0.0994 - val_rmse: 0.2599\n",
      "Epoch 2/100\n",
      "858835/858835 [==============================] - 18s 21us/step - loss: 0.0993 - mean_squared_error: 0.0993 - rmse: 0.2563 - val_loss: 0.0989 - val_mean_squared_error: 0.0989 - val_rmse: 0.2567\n",
      "Epoch 3/100\n",
      "858835/858835 [==============================] - 18s 20us/step - loss: 0.0990 - mean_squared_error: 0.0990 - rmse: 0.2556 - val_loss: 0.0985 - val_mean_squared_error: 0.0985 - val_rmse: 0.2538\n",
      "Epoch 4/100\n",
      "858835/858835 [==============================] - 18s 21us/step - loss: 0.0987 - mean_squared_error: 0.0987 - rmse: 0.2552 - val_loss: 0.0990 - val_mean_squared_error: 0.0990 - val_rmse: 0.2508\n",
      "Epoch 5/100\n",
      "858835/858835 [==============================] - 18s 21us/step - loss: 0.0986 - mean_squared_error: 0.0986 - rmse: 0.2549 - val_loss: 0.0984 - val_mean_squared_error: 0.0984 - val_rmse: 0.2522\n",
      "Epoch 6/100\n",
      "858835/858835 [==============================] - 18s 21us/step - loss: 0.0985 - mean_squared_error: 0.0985 - rmse: 0.2547 - val_loss: 0.0982 - val_mean_squared_error: 0.0982 - val_rmse: 0.2533\n",
      "Epoch 7/100\n",
      "858835/858835 [==============================] - 18s 21us/step - loss: 0.0984 - mean_squared_error: 0.0984 - rmse: 0.2546 - val_loss: 0.0983 - val_mean_squared_error: 0.0983 - val_rmse: 0.2543\n",
      "Epoch 8/100\n",
      "858835/858835 [==============================] - 18s 21us/step - loss: 0.0983 - mean_squared_error: 0.0983 - rmse: 0.2544 - val_loss: 0.0982 - val_mean_squared_error: 0.0982 - val_rmse: 0.2547\n",
      "Epoch 9/100\n",
      "858835/858835 [==============================] - 19s 22us/step - loss: 0.0983 - mean_squared_error: 0.0983 - rmse: 0.2544 - val_loss: 0.0982 - val_mean_squared_error: 0.0982 - val_rmse: 0.2553\n",
      "Epoch 10/100\n",
      "858835/858835 [==============================] - 19s 22us/step - loss: 0.0983 - mean_squared_error: 0.0983 - rmse: 0.2543 - val_loss: 0.0982 - val_mean_squared_error: 0.0982 - val_rmse: 0.2536\n",
      "Epoch 11/100\n",
      "858835/858835 [==============================] - 19s 22us/step - loss: 0.0982 - mean_squared_error: 0.0982 - rmse: 0.2542 - val_loss: 0.0981 - val_mean_squared_error: 0.0981 - val_rmse: 0.2544\n",
      "Epoch 12/100\n",
      "858835/858835 [==============================] - 19s 22us/step - loss: 0.0982 - mean_squared_error: 0.0982 - rmse: 0.2542 - val_loss: 0.0981 - val_mean_squared_error: 0.0981 - val_rmse: 0.2556\n",
      "Epoch 13/100\n",
      "858835/858835 [==============================] - 19s 22us/step - loss: 0.0982 - mean_squared_error: 0.0982 - rmse: 0.2542 - val_loss: 0.0983 - val_mean_squared_error: 0.0983 - val_rmse: 0.2529\n",
      "Epoch 14/100\n",
      "858835/858835 [==============================] - 20s 23us/step - loss: 0.0981 - mean_squared_error: 0.0981 - rmse: 0.2541 - val_loss: 0.0981 - val_mean_squared_error: 0.0981 - val_rmse: 0.2534\n",
      "Epoch 15/100\n",
      "858835/858835 [==============================] - 20s 23us/step - loss: 0.0981 - mean_squared_error: 0.0981 - rmse: 0.2540 - val_loss: 0.0983 - val_mean_squared_error: 0.0983 - val_rmse: 0.2520\n",
      "Epoch 16/100\n",
      "858835/858835 [==============================] - 20s 24us/step - loss: 0.0981 - mean_squared_error: 0.0981 - rmse: 0.2540 - val_loss: 0.0983 - val_mean_squared_error: 0.0983 - val_rmse: 0.2537\n",
      "Epoch 17/100\n",
      "858835/858835 [==============================] - 21s 24us/step - loss: 0.0981 - mean_squared_error: 0.0981 - rmse: 0.2540 - val_loss: 0.0980 - val_mean_squared_error: 0.0980 - val_rmse: 0.2533\n",
      "Epoch 18/100\n",
      "858835/858835 [==============================] - 20s 24us/step - loss: 0.0980 - mean_squared_error: 0.0980 - rmse: 0.2539 - val_loss: 0.0982 - val_mean_squared_error: 0.0982 - val_rmse: 0.2557\n",
      "Epoch 19/100\n",
      "858835/858835 [==============================] - 21s 25us/step - loss: 0.0980 - mean_squared_error: 0.0980 - rmse: 0.2539 - val_loss: 0.0981 - val_mean_squared_error: 0.0981 - val_rmse: 0.2551\n",
      "Epoch 20/100\n",
      "858835/858835 [==============================] - 21s 24us/step - loss: 0.0980 - mean_squared_error: 0.0980 - rmse: 0.2539 - val_loss: 0.0982 - val_mean_squared_error: 0.0982 - val_rmse: 0.2530\n",
      "Epoch 21/100\n",
      "858835/858835 [==============================] - 21s 24us/step - loss: 0.0980 - mean_squared_error: 0.0980 - rmse: 0.2539 - val_loss: 0.0984 - val_mean_squared_error: 0.0984 - val_rmse: 0.2571\n",
      "Epoch 22/100\n",
      "858835/858835 [==============================] - 21s 24us/step - loss: 0.0980 - mean_squared_error: 0.0980 - rmse: 0.2539 - val_loss: 0.0979 - val_mean_squared_error: 0.0979 - val_rmse: 0.2526\n",
      "Epoch 23/100\n",
      "858835/858835 [==============================] - 21s 24us/step - loss: 0.0980 - mean_squared_error: 0.0980 - rmse: 0.2538 - val_loss: 0.0986 - val_mean_squared_error: 0.0986 - val_rmse: 0.2596\n",
      "Epoch 24/100\n",
      "858835/858835 [==============================] - 21s 25us/step - loss: 0.0979 - mean_squared_error: 0.0979 - rmse: 0.2538 - val_loss: 0.0979 - val_mean_squared_error: 0.0979 - val_rmse: 0.2538\n",
      "Epoch 25/100\n",
      "858835/858835 [==============================] - 21s 25us/step - loss: 0.0979 - mean_squared_error: 0.0979 - rmse: 0.2538 - val_loss: 0.0981 - val_mean_squared_error: 0.0981 - val_rmse: 0.2542\n",
      "Epoch 26/100\n",
      "858835/858835 [==============================] - 22s 25us/step - loss: 0.0979 - mean_squared_error: 0.0979 - rmse: 0.2537 - val_loss: 0.0982 - val_mean_squared_error: 0.0982 - val_rmse: 0.2514\n",
      "Epoch 27/100\n",
      "858835/858835 [==============================] - 21s 25us/step - loss: 0.0979 - mean_squared_error: 0.0979 - rmse: 0.2537 - val_loss: 0.0980 - val_mean_squared_error: 0.0980 - val_rmse: 0.2530\n",
      "Epoch 28/100\n",
      "858835/858835 [==============================] - 21s 25us/step - loss: 0.0979 - mean_squared_error: 0.0979 - rmse: 0.2537 - val_loss: 0.0980 - val_mean_squared_error: 0.0980 - val_rmse: 0.2532\n",
      "Epoch 29/100\n",
      "858835/858835 [==============================] - 21s 25us/step - loss: 0.0979 - mean_squared_error: 0.0979 - rmse: 0.2537 - val_loss: 0.0979 - val_mean_squared_error: 0.0979 - val_rmse: 0.2532\n",
      "Epoch 30/100\n",
      "858835/858835 [==============================] - 21s 24us/step - loss: 0.0979 - mean_squared_error: 0.0979 - rmse: 0.2536 - val_loss: 0.0979 - val_mean_squared_error: 0.0979 - val_rmse: 0.2541\n",
      "Epoch 31/100\n",
      "858835/858835 [==============================] - 21s 25us/step - loss: 0.0979 - mean_squared_error: 0.0979 - rmse: 0.2536 - val_loss: 0.0979 - val_mean_squared_error: 0.0979 - val_rmse: 0.2540\n",
      "Epoch 32/100\n",
      "858835/858835 [==============================] - 21s 25us/step - loss: 0.0978 - mean_squared_error: 0.0978 - rmse: 0.2536 - val_loss: 0.0982 - val_mean_squared_error: 0.0982 - val_rmse: 0.2553\n",
      "Epoch 33/100\n",
      "858835/858835 [==============================] - 21s 25us/step - loss: 0.0978 - mean_squared_error: 0.0978 - rmse: 0.2536 - val_loss: 0.0979 - val_mean_squared_error: 0.0979 - val_rmse: 0.2525\n",
      "Epoch 34/100\n",
      "858835/858835 [==============================] - 21s 25us/step - loss: 0.0978 - mean_squared_error: 0.0978 - rmse: 0.2536 - val_loss: 0.0979 - val_mean_squared_error: 0.0979 - val_rmse: 0.2538\n",
      "Epoch 35/100\n",
      "858835/858835 [==============================] - 21s 24us/step - loss: 0.0978 - mean_squared_error: 0.0978 - rmse: 0.2535 - val_loss: 0.0979 - val_mean_squared_error: 0.0979 - val_rmse: 0.2547\n",
      "Epoch 36/100\n",
      "858835/858835 [==============================] - 21s 24us/step - loss: 0.0978 - mean_squared_error: 0.0978 - rmse: 0.2535 - val_loss: 0.0980 - val_mean_squared_error: 0.0980 - val_rmse: 0.2551\n",
      "Epoch 37/100\n",
      "858835/858835 [==============================] - 21s 24us/step - loss: 0.0978 - mean_squared_error: 0.0978 - rmse: 0.2535 - val_loss: 0.0979 - val_mean_squared_error: 0.0979 - val_rmse: 0.2516\n",
      "Epoch 38/100\n",
      "858835/858835 [==============================] - 21s 24us/step - loss: 0.0978 - mean_squared_error: 0.0978 - rmse: 0.2535 - val_loss: 0.0979 - val_mean_squared_error: 0.0979 - val_rmse: 0.2547\n",
      "Epoch 39/100\n",
      "837750/858835 [============================>.] - ETA: 0s - loss: 0.0978 - mean_squared_error: 0.0978 - rmse: 0.2535"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100, batch_size=50,  verbose=1, validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred= model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = scaler_y.inverse_transform(y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = scaler_y.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE value is: 81211.81519164734\n"
     ]
    }
   ],
   "source": [
    "error = sqrt(mean_squared_error(pred,real))\n",
    "\n",
    "print('RMSE value is:', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"keras_auctions_reducido.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediccion competencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_competencia = pd.read_pickle(\"../../../analisis/solo_competencia/armado_features/features_completos_competencia.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_competencia.fillna(0,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4037, 102)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_competencia.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cantidad_apariciones', 'apariciones_en_hora_0',\n",
       "       'apariciones_en_hora_1', 'apariciones_en_hora_2',\n",
       "       'apariciones_en_hora_3', 'apariciones_en_hora_4',\n",
       "       'apariciones_en_hora_5', 'apariciones_en_hora_6',\n",
       "       'apariciones_en_hora_7', 'apariciones_en_hora_8',\n",
       "       ...\n",
       "       'clicks_en_hora_21', 'clicks_en_hora_22', 'clicks_en_hora_23',\n",
       "       'clicks_en_dia_1', 'clicks_en_dia_2', 'clicks_en_dia_3',\n",
       "       'tiempo_medio_entre_clicks', 'cantidad_instalaciones',\n",
       "       'tiempo_ultima_instalacion_hasta_fin_ventana',\n",
       "       'cantidad_aplicaciones_diferentes'],\n",
       "      dtype='object', length=103)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = features_competencia.iloc[1,:24].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "You have to supply one of 'by' and 'level'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-8de6296bc1bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatures_competencia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"device_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"apariciones_en_hora_{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhora\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhora\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sum\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, **kwargs)\u001b[0m\n\u001b[1;32m   7626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7627\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mby\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7628\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to supply one of 'by' and 'level'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7629\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7630\u001b[0m         return groupby(self, by=by, axis=axis, level=level, as_index=as_index,\n",
      "\u001b[0;31mTypeError\u001b[0m: You have to supply one of 'by' and 'level'"
     ]
    }
   ],
   "source": [
    "features_competencia.groupby([\"device_id\"].append([\"apariciones_en_hora_{}\".format(hora) for hora in range(0,24)])).agg(\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_x_comp = MinMaxScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_comp =scaler_x_comp.fit_transform(features_competencia)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "real2 = []\n",
    "for valor in real:\n",
    "    real2.append(valor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = []\n",
    "for valor in pred:\n",
    "    pred2.append(valor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE value is: 46782.89622217884\n"
     ]
    }
   ],
   "source": [
    "error = sqrt(mean_squared_error(pred2,real2))\n",
    "\n",
    "print('RMSE value is:', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>254071.625000</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>250365.828125</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>250677.875000</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>254244.421875</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>130438.468750</td>\n",
       "      <td>6885.802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>253830.453125</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>253495.984375</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>251706.750000</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>126759.046875</td>\n",
       "      <td>58330.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>253883.453125</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>254606.296875</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>253890.515625</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>254033.000000</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>254545.234375</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>248110.359375</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>252842.343750</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>253453.609375</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>251201.171875</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>124572.859375</td>\n",
       "      <td>251329.528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>254243.656250</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>254595.515625</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>254673.109375</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>130871.671875</td>\n",
       "      <td>157004.823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>259229.187500</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>254475.984375</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>254298.703125</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>254165.718750</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>254342.234375</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>250820.531250</td>\n",
       "      <td>107940.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>254613.000000</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147485</th>\n",
       "      <td>253645.937500</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147486</th>\n",
       "      <td>253896.328125</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147487</th>\n",
       "      <td>254247.718750</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147488</th>\n",
       "      <td>254625.265625</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147489</th>\n",
       "      <td>246235.250000</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147490</th>\n",
       "      <td>254466.750000</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147491</th>\n",
       "      <td>126846.382812</td>\n",
       "      <td>29903.955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147492</th>\n",
       "      <td>253718.890625</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147493</th>\n",
       "      <td>254544.171875</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147494</th>\n",
       "      <td>128345.734375</td>\n",
       "      <td>13029.522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147495</th>\n",
       "      <td>254591.531250</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147496</th>\n",
       "      <td>253837.125000</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147497</th>\n",
       "      <td>245426.656250</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147498</th>\n",
       "      <td>135537.375000</td>\n",
       "      <td>172094.437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147499</th>\n",
       "      <td>254644.515625</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147500</th>\n",
       "      <td>250777.515625</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147501</th>\n",
       "      <td>254551.031250</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147502</th>\n",
       "      <td>130438.468750</td>\n",
       "      <td>59264.239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147503</th>\n",
       "      <td>253134.875000</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147504</th>\n",
       "      <td>253657.453125</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147505</th>\n",
       "      <td>254669.343750</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147506</th>\n",
       "      <td>124639.265625</td>\n",
       "      <td>194662.671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147507</th>\n",
       "      <td>125306.320312</td>\n",
       "      <td>6907.243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147508</th>\n",
       "      <td>254432.859375</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147509</th>\n",
       "      <td>252354.500000</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147510</th>\n",
       "      <td>251663.531250</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147511</th>\n",
       "      <td>124028.742188</td>\n",
       "      <td>179345.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147512</th>\n",
       "      <td>253056.562500</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147513</th>\n",
       "      <td>253004.156250</td>\n",
       "      <td>259200.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147514</th>\n",
       "      <td>124135.101562</td>\n",
       "      <td>67690.731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147515 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 pred        real\n",
       "0       254071.625000  259200.000\n",
       "1       250365.828125  259200.000\n",
       "2       250677.875000  259200.000\n",
       "3       254244.421875  259200.000\n",
       "4       130438.468750    6885.802\n",
       "5       253830.453125  259200.000\n",
       "6       253495.984375  259200.000\n",
       "7       251706.750000  259200.000\n",
       "8       126759.046875   58330.013\n",
       "9       253883.453125  259200.000\n",
       "10      254606.296875  259200.000\n",
       "11      253890.515625  259200.000\n",
       "12      254033.000000  259200.000\n",
       "13      254545.234375  259200.000\n",
       "14      248110.359375  259200.000\n",
       "15      252842.343750  259200.000\n",
       "16      253453.609375  259200.000\n",
       "17      251201.171875  259200.000\n",
       "18      124572.859375  251329.528\n",
       "19      254243.656250  259200.000\n",
       "20      254595.515625  259200.000\n",
       "21      254673.109375  259200.000\n",
       "22      130871.671875  157004.823\n",
       "23      259229.187500  259200.000\n",
       "24      254475.984375  259200.000\n",
       "25      254298.703125  259200.000\n",
       "26      254165.718750  259200.000\n",
       "27      254342.234375  259200.000\n",
       "28      250820.531250  107940.938\n",
       "29      254613.000000  259200.000\n",
       "...               ...         ...\n",
       "147485  253645.937500  259200.000\n",
       "147486  253896.328125  259200.000\n",
       "147487  254247.718750  259200.000\n",
       "147488  254625.265625  259200.000\n",
       "147489  246235.250000  259200.000\n",
       "147490  254466.750000  259200.000\n",
       "147491  126846.382812   29903.955\n",
       "147492  253718.890625  259200.000\n",
       "147493  254544.171875  259200.000\n",
       "147494  128345.734375   13029.522\n",
       "147495  254591.531250  259200.000\n",
       "147496  253837.125000  259200.000\n",
       "147497  245426.656250  259200.000\n",
       "147498  135537.375000  172094.437\n",
       "147499  254644.515625  259200.000\n",
       "147500  250777.515625  259200.000\n",
       "147501  254551.031250  259200.000\n",
       "147502  130438.468750   59264.239\n",
       "147503  253134.875000  259200.000\n",
       "147504  253657.453125  259200.000\n",
       "147505  254669.343750  259200.000\n",
       "147506  124639.265625  194662.671\n",
       "147507  125306.320312    6907.243\n",
       "147508  254432.859375  259200.000\n",
       "147509  252354.500000  259200.000\n",
       "147510  251663.531250  259200.000\n",
       "147511  124028.742188  179345.230\n",
       "147512  253056.562500  259200.000\n",
       "147513  253004.156250  259200.000\n",
       "147514  124135.101562   67690.731\n",
       "\n",
       "[147515 rows x 2 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"pred\":pred2,\"real\":real2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
